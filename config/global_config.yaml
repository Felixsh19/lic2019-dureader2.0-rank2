global:
  random_seed: 42
  gpu: 3                # GPU id for training and testing
  model: match_lstm    # 'match-lstm', 'match-lstm+', 'r-net', 'm-reader' or 'qanet'

data:
  data_type: demo
  mrc_dataset:
    train_path: input/demo/search.train.json
    dev_path: input/demo/search.dev.json
    test_path: input/dureader_2.0_v4/mrc_dataset/testset/search.test1.json

  data_cache_dir: input/demo/cache
  train_badcase_save_path: ./logs/
  embeddings_file: ../../pretrained_embeddings/chinese/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5
  max_p_num: 5
  max_p_len: 500
  max_q_len: 60
  min_word_cnt: 2

bidaf:  # BiDAF model config
  embed_dim: 300

train:
  optimizer: 'sgd'  # adam, sgd, adamax, adadelta(default is adamax)
  learning_rate: 0.01  # only for sgd
  clip_grad_norm: 100

  batch_size: 16
  valid_batch_size: 64
