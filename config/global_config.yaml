global:
  random_seed: 42
  gpu: 2                # GPU id for training and testing
  model: match_lstm    # 'match-lstm', 'match-lstm+', 'r-net', 'm-reader' or 'qanet'

data:
  data_type: search
  mrc_dataset:
    train_path: input/dureader_2.0_v4/mrc_dataset/trainset/search.train.json
    dev_path: input/dureader_2.0_v4/mrc_dataset/devset/search.dev.json
    test_path: input/dureader_2.0_v4/mrc_dataset/testset/search.test1.json

  data_cache_dir: input/dureader_2.0_v4/cache
  train_badcase_save_path: ./logs/
  embeddings_file: ../../pretrained_embeddings/chinese/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5
  max_p_num: 5
  max_p_len: 500
  max_q_len: 60
  min_word_cnt: 2

bidaf:  # BiDAF model config
  embed_dim: 300

train:
  optimizer: 'adam'  # adam, sgd, adamax, adadelta(default is adamax)
  learning_rate: 0.002  # only for sgd
  clip_grad_norm: 5

  batch_size: 32
  valid_batch_size: 64
