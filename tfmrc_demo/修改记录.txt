4月11日:
  my_vocab.py: 类初始化时增加参数init_random=False, 可以选择是否随机初始化<splitter>, <blank>, <unk>
  run.py: 增加了一些额外的参数, 包括特征的选取, loss的选择等
  my_model.py:实现了2018年第一名论文的部分思想,主要为POS特征, keyword特征(自己加的), wiq特征, 多答案损失函数, 段落选择损失函数
  loss_func.py: 目前包括了baseline的单答案损失, multi_ans_loss和段落选择损失, 该版本暂时没完成最小风险损失
  my_dataset.py: 根据自己做的第一版处理的数据,重新定义了数据结构,同样在my_model.py中也有相应修改
  测试样本数据流程正常! 注意用的vocab.filter_tokens_by_cnt(min_cnt=20), 20还可以调整


4月12日:
  加入了百度paddle模型中有的para_prior_scores=(0.44, 0.23, 0.15, 0.09, 0.07), 目前可以通过--use_para_prior_scores
    来决定是否使用该分数,目前分数固定为(0.44, 0.23, 0.15, 0.09, 0.07)
  修改了match_score的归一化方式(包括BUG), 不再使用softmax,而是直接除以所有分数的和归一化,另外除以0时调整为0,而不是nan

4月13日:
  目前,还有dataset部分的wiq feature没有完成; 还有POS feature没有进行字符串到数字的映射

4月14日
  更改了vocab.py和dataset.py,重新调整了数据

4月15
  修正了evaluate函数

4月15晚
  修改了vocab, 命名为optimized_vocab.py
  修改了dataset中test时候没有answer_labels的bug

4月16日:
  发现tf10没有div_no_nan函数,以后需要进行修复
  再次修改了vocab
  更改了use_para_prior_scores的使用,现在不是bool, 而是一个str
  写了后处理的文件

4月17:
  修改了模型self.vocab.oov_word_end_idx

4月17晚:
  增加 TODO:分数是否要进行归一化处理?

4月18:
  run.py中,将extra_settings重新整理, 现在对于vocab, 需要指定其路径以及文件

4月19:
  TODO pos freq需要集成; 已完成

4月24:
TODO:粗分类细分类还未完成

4月26:
  
